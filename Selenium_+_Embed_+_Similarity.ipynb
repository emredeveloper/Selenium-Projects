{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmTujv8olqtWA+MRbJIvvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emredeveloper/Selenium-Projects/blob/main/Selenium_%2B_Embed_%2B_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLAUDE 3.5 SONNET ile yapılmış sorgu verilmiştir"
      ],
      "metadata": {
        "id": "mTEBkLM7uHYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia Veri Bilimi Sayfası Analizi\n",
        "\n",
        "Bu script aşağıdaki işlemleri gerçekleştirir:\n",
        "\n",
        "1. **Argüman Ayrıştırma**\n",
        "   - Kullanıcıdan başlık sayısı, dil ve konu bilgilerini alır.\n",
        "\n",
        "2. **Wikipedia'dan Veri Çekme**\n",
        "   - Selenium kullanarak belirtilen Wikipedia sayfasını açar.\n",
        "   - İstenilen sayıda başlık ve ilk cümleleri çeker.\n",
        "\n",
        "3. **Veri Ön İşleme**\n",
        "   - Çekilen metinleri küçük harfe çevirir.\n",
        "   - Noktalama işaretlerini kaldırır.\n",
        "   - Stop words'leri temizler.\n",
        "\n",
        "4. **Embedding Oluşturma**\n",
        "   - Sentence Transformers kullanarak metinleri vektörlere dönüştürür.\n",
        "\n",
        "5. **Benzerlik Analizi**\n",
        "   - Cosine similarity kullanarak cümleler arası benzerliği hesaplar.\n",
        "   - Benzerlik matrisini görselleştirir.\n",
        "   - En benzer iki cümleyi bulur.\n",
        "\n",
        "6. **Kelime Bulutu Oluşturma**\n",
        "   - Tüm metinlerden bir kelime bulutu oluşturur ve görselleştirir.\n",
        "\n",
        "7. **Kelime Frekans Analizi**\n",
        "   - En sık kullanılan 10 kelimeyi ve frekanslarını hesaplar.\n",
        "\n",
        "8. **Duygu Analizi**\n",
        "   - TextBlob kullanarak her cümlenin duygu polaritesini hesaplar.\n",
        "   - Ortalama duygu polaritesini hesaplar.\n",
        "\n",
        "9. **Kümeleme Analizi**\n",
        "   - K-means algoritması kullanarak cümleleri kümeler.\n",
        "   - Kümeleme sonuçlarını yazdırır.\n",
        "\n",
        "10. **Ortalama Benzerlik Hesaplama**\n",
        "    - Her cümlenin diğer cümlelerle ortalama benzerliğini hesaplar.\n",
        "\n",
        "11. **Sonuçları Kaydetme**\n",
        "    - Tüm analiz sonuçlarını bir CSV dosyasına kaydeder.\n",
        "\n",
        "12. **Ek Görselleştirmeler**\n",
        "    - Kümelenmiş embedding'leri görselleştirir.\n",
        "    - Duygu analizi sonuçlarını görselleştirir.\n",
        "\n",
        "13. **Loglama**\n",
        "    - Tüm işlem adımlarını loglar."
      ],
      "metadata": {
        "id": "3FbirrQzTyoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import argparse\n",
        "import csv\n",
        "import string\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from wordcloud import WordCloud\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Argüman ayrıştırıcı ekle\n",
        "parser = argparse.ArgumentParser(description='Wikipedia Veri Bilimi sayfası analizi')\n",
        "parser.add_argument('--num_topics', type=int, default=5, help='Analiz edilecek başlık sayısı')\n",
        "parser.add_argument('--language', type=str, default='en', help='Wikipedia dili (örn. \"en\" veya \"tr\")')\n",
        "parser.add_argument('--topic', type=str, default='Data_science', help='Wikipedia konusu (örn. \"Data_science\" veya \"Veri_bilimi\")')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Loglama ayarla\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# NLTK için gerekli verileri indir\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Metin ön işleme fonksiyonu\n",
        "def preprocess_text(text, lang='english'):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    stop_words = set(stopwords.words(lang))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [w for w in word_tokens if w not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "# Chrome options ayarla\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "\n",
        "# WebDriver'ı başlat\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Wikipedia URL'sini dile ve konuya göre ayarla\n",
        "wiki_url = f\"https://{args.language}.wikipedia.org/wiki/{args.topic}\"\n",
        "driver.get(wiki_url)\n",
        "\n",
        "logging.info(f\"Wikipedia sayfası açıldı: {wiki_url}\")\n",
        "\n",
        "# İlk n başlığı ve ilk cümleleri saklamak için bir liste\n",
        "topics = []\n",
        "\n",
        "# Başlıkları bul\n",
        "headers = WebDriverWait(driver, 10).until(\n",
        "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"h2 .mw-headline\"))\n",
        ")\n",
        "\n",
        "# İlk n başlık için döngü (n, komut satırı argümanından gelir)\n",
        "for i, header in enumerate(headers[:args.num_topics], 1):\n",
        "    title = header.text\n",
        "    # Başlığın altındaki ilk paragrafı bul\n",
        "    try:\n",
        "        paragraph = WebDriverWait(driver, 5).until(\n",
        "            EC.presence_of_element_located((By.XPATH, f\"//h2[span[@class='mw-headline' and text()='{title}']]//following-sibling::p[1]\"))\n",
        "        )\n",
        "        first_sentence = paragraph.text.split('.')[0] + '.'  # İlk cümleyi al\n",
        "        topics.append([title, first_sentence])\n",
        "        logging.info(f\"Başlık {i} çekildi: {title}\")\n",
        "    except:\n",
        "        logging.warning(f\"Başlık {i} için paragraf bulunamadı: {title}\")\n",
        "\n",
        "# Tarayıcıyı kapat\n",
        "driver.quit()\n",
        "\n",
        "# CSV dosyasına yaz\n",
        "with open('data_science_topics.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Title\", \"First Sentence\"])\n",
        "    writer.writerows(topics)\n",
        "\n",
        "logging.info(\"Veriler CSV dosyasına kaydedildi.\")\n",
        "\n",
        "# Cümleleri ayır\n",
        "sentences = [topic[1] for topic in topics]\n",
        "\n",
        "# Metinleri ön işle\n",
        "preprocessed_sentences = [preprocess_text(sentence, 'english' if args.language == 'en' else 'turkish') for sentence in tqdm(sentences, desc=\"Preprocessing\")]\n",
        "\n",
        "# Embedding modeli yükle\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Ön işlenmiş metinleri embed et\n",
        "embeddings = model.encode(preprocessed_sentences, show_progress_bar=True)\n",
        "\n",
        "# Benzerlik matrisini hesapla\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Benzerlik matrisini görselleştir\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Similarity Matrix Heatmap')\n",
        "plt.savefig('similarity_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "logging.info(\"Benzerlik matrisi oluşturuldu ve kaydedildi.\")\n",
        "\n",
        "# En benzer iki cümleyi bul\n",
        "max_similarity = np.max(np.triu(similarity_matrix, k=1))\n",
        "most_similar_pair = np.unravel_index(np.argmax(np.triu(similarity_matrix, k=1)), similarity_matrix.shape)\n",
        "\n",
        "print(f\"\\nEn benzer iki cümle:\")\n",
        "print(f\"1. {sentences[most_similar_pair[0]]}\")\n",
        "print(f\"2. {sentences[most_similar_pair[1]]}\")\n",
        "print(f\"Benzerlik: {max_similarity:.2f}\")\n",
        "\n",
        "# Kelime bulutu oluştur\n",
        "all_text = ' '.join(preprocessed_sentences)\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Data Science Topics')\n",
        "plt.savefig('wordcloud.png')\n",
        "plt.close()\n",
        "\n",
        "logging.info(\"Kelime bulutu oluşturuldu ve kaydedildi.\")\n",
        "\n",
        "# En sık kullanılan kelimeleri bul\n",
        "words = all_text.split()\n",
        "word_freq = Counter(words)\n",
        "print(\"\\nEn sık kullanılan 10 kelime:\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Duygu analizi\n",
        "sentiments = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
        "avg_sentiment = sum(sentiments) / len(sentiments)\n",
        "print(f\"\\nOrtalama duygu polaritesi: {avg_sentiment:.2f}\")\n",
        "\n",
        "# K-means kümeleme\n",
        "num_clusters = min(3, len(sentences))  # En fazla 3 küme veya cümle sayısı kadar\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Kümeleme sonuçlarını yazdır\n",
        "for i in range(num_clusters):\n",
        "    print(f\"\\nKüme {i+1}:\")\n",
        "    cluster_sentences = [sentences[j] for j in range(len(sentences)) if cluster_labels[j] == i]\n",
        "    for sentence in cluster_sentences:\n",
        "        print(f\"- {sentence}\")\n",
        "\n",
        "# Daha fazla analiz: Her cümlenin ortalama benzerliği\n",
        "avg_similarities = np.mean(similarity_matrix, axis=1)\n",
        "for i, avg_sim in enumerate(avg_similarities):\n",
        "    print(f\"Cümle {i+1} ortalama benzerlik: {avg_sim:.2f}\")\n",
        "\n",
        "# Sonuçları DataFrame'e dönüştür ve CSV'ye kaydet\n",
        "results_df = pd.DataFrame({\n",
        "    'Title': [topic[0] for topic in topics],\n",
        "    'Sentence': sentences,\n",
        "    'Preprocessed_Sentence': preprocessed_sentences,\n",
        "    'Average_Similarity': avg_similarities,\n",
        "    'Cluster': cluster_labels,\n",
        "    'Sentiment': sentiments\n",
        "})\n",
        "# Embed değerlerini ekle\n",
        "for i in range(embeddings.shape[1]):\n",
        "    results_df[f'Embed_{i+1}'] = embeddings[:, i]\n",
        "results_df.to_csv('data_science_analysis.csv', index=False)\n",
        "\n",
        "logging.info(\"Sonuçlar 'data_science_analysis.csv' dosyasına kaydedildi.\")\n",
        "\n",
        "# Ek görselleştirmeler\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=embeddings[:, 0], y=embeddings[:, 1], hue=cluster_labels, palette='deep')\n",
        "plt.title('Sentence Embeddings Clustered')\n",
        "plt.savefig('embedding_clusters.png')\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(range(len(sentences))), y=sentiments)\n",
        "plt.title('Sentiment Analysis of Sentences')\n",
        "plt.xlabel('Sentence Index')\n",
        "plt.ylabel('Sentiment Polarity')\n",
        "plt.savefig('sentiment_analysis.png')\n",
        "plt.close()\n",
        "\n",
        "logging.info(\"Analiz tamamlandı. Tüm sonuçlar ve görseller kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_dwz_5CvS0b",
        "outputId": "ed312460-4d4f-4982-a417-59dbd08db844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py --num_topics 7 --language tr --topic Veri_bilimi"
      ],
      "metadata": {
        "id": "WO9tHJxAvb4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qwoY9Kjw9KY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}